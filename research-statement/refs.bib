@inproceedings{billings_understanding_2010,
  title = {Understanding Dispute Resolution Online: Using Text to Reflect Personal and Substantive Issues in Conflict},
  booktitle = {Proceedings of {{CHI}}},
  author = {Billings, Matt and Watts, Leon A.},
  year = {2010},
  month = apr,
  urldate = {2021-10-25},
  abstract = {Conflict is a natural part of human communication with implications for the work and well-being of a community. It can cause projects to stall or fail. Alternatively new insights can be produced that are valuable to the community, and membership can be strengthened. We describe how Wikipedia mediators create and maintain a 'safe space'. They help conflicting parties to express, recognize and respond positively to their personal and substantive differences. We show how the 'mutability' of wiki text can be used productively by mediators: to legitimize and restructure the personal and substantive issues under dispute; to actively and visibly differentiate personal from substantive elements in the dispute, and to maintain asynchronous engagement by adjusting expectations of timeliness. We argue that online conflicts could be effectively conciliated in other text-based web communities, provided power differences can be controlled, by policies and technical measures for maintaining special 'safe' conflict resolution spaces.}
}

@inproceedings{brewer_inclusion_2020,
  title = {Inclusion at {{Scale}}: {{Deploying}} a {{Community-Driven Moderation Intervention}} on {{Twitch}}},
  booktitle = {Proceedings of {{DIS}}},
  author = {Brewer, Johanna and Romine, Morgan and Taylor, T. L.},
  year = {2020},
  month = jul,
  urldate = {2021-12-10},
  abstract = {Harassment, especially of marginalized individuals, on networked gaming and social media platforms has been identified as a significant issue, yet few HCI practitioners have attempted to create interventions tackling toxicity online. Aligning ourselves with the growing cohort of design activists, we present a case study of the GLHF pledge, an interactive public awareness campaign promoting positivity in video game live streaming. We discuss the design and deployment of a community-driven moderation intervention for GLHF, intended to empower the inclusive communities emerging on Twitch. After offering a preliminary report on the effects we have observed based on the more than 370,000 gamers who have participated to date, the paper concludes with a reflection on the challenges and opportunities of using design activism to positively intervene in large-scale media platforms.}
}

@inproceedings{cai_what_2019,
  title = {What Are {{Effective Strategies}} of {{Handling Harassment}} on {{Twitch}}? {{Users}}' {{Perspectives}}},
  booktitle = {Proceedings of {{CSCW}}},
  author = {Cai, Jie and Wohn, Donghee Yvette},
  year = {2019},
  month = nov,
  urldate = {2021-10-25},
  abstract = {Harassment is an issue in online communities with the live streaming platform Twitch being no exception. In this study, we surveyed 375 Twitch users in person at TwitchCon, asking them about who should be responsible for deciding what should be allowed and what strategies they perceived to be effective in handling harassment. We found that users thought that streamers should be most responsible for enforcing rules and that either blocking bad actors, ignoring them, or trying to educate them were the most effective strategies.}
}

@inproceedings{chandrasekharan_internets_2018,
  title = {The {{Internet}}'s {{Hidden Rules}}: {{An Empirical Study}} of {{Reddit Norm Violations}} at {{Micro}}, {{Meso}}, and {{Macro Scales}}},
  booktitle = {Proceedings of {{CSCW}}},
  author = {Chandrasekharan, Eshwar and Samory, Mattia and Jhaver, Shagun and Charvat, Hunter and Bruckman, Amy and Lampe, Cliff and Eisenstein, Jacob and Gilbert, Eric},
  year = {2018},
  month = nov,
  urldate = {2018-11-08},
  abstract = {Norms are central to how online communities are governed. Yet, norms are also emergent, arise from interaction, and can vary significantly between communities---making them challenging to study at scale. In this paper, we study community norms on Reddit in a large-scale, empirical manner. Via 2.8M comments removed by moderators of 100 top subreddits over 10 months, we use both computational and qualitative methods to identify three types of norms: macro norms that are universal to most parts of Reddit; meso norms that are shared across certain groups of subreddits; and micro norms that are specific to individual, relatively unique subreddits. Given the size of Reddit's user base---and the wide range of topics covered by different subreddits---we argue this represents the first large-scale census of the norms in broader internet culture. In other words, these findings shed light on what Reddit values, and how widely-held those values are. We conclude by discussing implications for the design of new and existing online communities.}
}

@inproceedings{chang_convokit_2020,
  title = {{{ConvoKit}}: {{A Toolkit}} for the {{Analysis}} of {{Conversations}}},
  booktitle = {Proceedings of {{SIGDIAL}}},
  author = {Chang, Jonathan P. and Chiam, Caleb and Fu, Liye and Wang, Andrew and Zhang, Justine and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2020}
}

@inproceedings{chang_dont_2020,
  title = {Don't {{Let Me Be Misunderstood}}:{{Comparing Intentions}} and {{Perceptions}} in {{Online Discussions}}},
  booktitle = {Proceedings of {{WWW}}},
  author = {Chang, Jonathan P and Cheng, Justin and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2020},
  abstract = {Discourse involves two perspectives: a person's intention in making an utterance and others' perception of that utterance. The misalignment between these perspectives can lead to undesirable outcomes, such as misunderstandings, low productivity and even overt strife. In this work, we present a computational framework for exploring and comparing both perspectives in online public discussions. We combine logged data about public comments on Facebook with a survey of over 16,000 people about their intentions in writing these comments or about their perceptions of comments that others had written. Unlike previous studies of online discussions that have largely relied on third-party labels to quantify properties such as sentiment and subjectivity, our approach also directly captures what the speakers actually intended when writing their comments. In particular, our analysis focuses on judgments of whether a comment is stating a fact or an opinion, since these concepts were shown to be often confused.},
  langid = {english}
}

@article{chang_thread_2022,
  title = {Thread {{With Caution}}: {{Proactively Helping Users Assess}} and {{Deescalate Tension}} in {{Their Online Discussions}}},
  author = {Chang, Jonathan P. and Schluger, Charlotte and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2022},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW2},
  urldate = {2022-11-23},
  abstract = {Incivility remains a major challenge for online discussion platforms, to such an extent that even conversations between well-intentioned users can often derail into uncivil behavior. Traditionally, platforms have relied on moderators to---with or without algorithmic assistance---take corrective actions such as removing comments or banning users. In this work we propose a complementary paradigm that directly empowers users by proactively enhancing their awareness about existing tension in the conversation they are engaging in and actively guides them as they are drafting their replies to avoid further escalation. As a proof of concept for this paradigm, we design an algorithmic tool that provides such proactive information directly to users, and conduct a user study in a popular discussion platform. Through a mixed methods approach combining surveys with a randomized controlled experiment, we uncover qualitative and quantitative insights regarding how the participants utilize and react to this information. Most participants report finding this proactive paradigm valuable, noting that it helps them to identify tension that they may have otherwise missed and prompts them to further reflect on their own replies and to revise them. These effects are corroborated by a comparison of how the participants draft their reply when our tool warns them that their conversation is at risk of derailing into uncivil behavior versus in a control condition where the tool is disabled.These preliminary findings highlight the potential of this user-centered paradigm and point to concrete directions for future implementations.}
}

@inproceedings{chang_trajectories_2019,
  title = {Trajectories of {{Blocked Community Members}}: {{Redemption}}, {{Recidivism}} and {{Departure}}},
  booktitle = {Proceedings of {{WWW}}},
  author = {Chang, Jonathan P. and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2019},
  month = may,
  urldate = {2019-03-03},
  abstract = {Community norm violations can impair constructive communication and collaboration online. As a defense mechanism, community moderators often address such transgressions by temporarily blocking the perpetrator. Such actions, however, come with the cost of potentially alienating community members. Given this tradeoff, it is essential to understand to what extent, and in which situations, this common moderation practice is effective in reinforcing community rules. In this work, we introduce a computational framework for studying the future behavior of blocked users on Wikipedia. After their block expires, they can take several distinct paths: they can reform and adhere to the rules, but they can also recidivate, or straight-out abandon the community. We reveal that these trajectories are tied to factors rooted both in the characteristics of the blocked individual and in whether they perceived the block to be fair and justified. Based on these insights, we formulate a series of prediction tasks aiming to determine which of these paths a user is likely to take after being blocked for their first offense, and demonstrate the feasibility of these new tasks. Overall, this work builds towards a more nuanced approach to moderation by highlighting the tradeoffs that are in play.}
}

@inproceedings{chang_trouble_2019,
  title = {Trouble on the {{Horizon}}: {{Forecasting}} the {{Derailment}} of {{Online Conversations}} as They {{Develop}}},
  booktitle = {Proceedings of  {{EMNLP}}},
  author = {Chang, Jonathan P. and {Danescu-Niculescu-Mizil}, Cristian},
  year = {2019},
  urldate = {2019-09-25},
  abstract = {Online discussions often derail into toxic exchanges between participants. Recent efforts mostly focused on detecting antisocial behavior after the fact, by analyzing single comments in isolation. To provide more timely notice to human moderators, a system needs to preemptively detect that a conversation is heading towards derailment before it actually turns toxic. This means modeling derailment as an emerging property of a conversation rather than as an isolated utterance-level event. Forecasting emerging conversational properties, however, poses several inherent modeling challenges. First, since conversations are dynamic, a forecasting model needs to capture the flow of the discussion, rather than properties of individual comments. Second, real conversations have an unknown horizon: they can end or derail at any time; thus a practical forecasting model needs to assess the risk in an online fashion, as the conversation develops. In this work we introduce a conversational forecasting model that learns an unsupervised representation of conversational dynamics and exploits it to predict future derailment as the conversation develops. By applying this model to two new diverse datasets of online conversations with labels for antisocial events, we show that it outperforms state-of-the-art systems at forecasting derailment.}
}

@inproceedings{gilbert_i_2020,
  title = {"{{I}} Run the World's Largest Historical Outreach Project and It's on a Cesspool of a Website." {{Moderating}} a {{Public Scholarship Site}} on {{Reddit}}: {{A Case Study}} of r/{{AskHistorians}}},
  booktitle = {Proceedings of {{CSCW}}},
  author = {Gilbert, Sarah A.},
  year = {2020},
  month = may,
  urldate = {2020-10-19},
  abstract = {Online communities provide important functions in their participants' lives, from providing spaces to discuss topics of interest to supporting the development of close, personal relationships. Volunteer moderators play key roles in maintaining these spaces, such as creating and enforcing rules and modeling normative behavior. While these users play important governance roles in online spaces, less is known about how the work they do is impacted by platform design and culture. r/AskHistorians, a Reddit-based question and answer forum dedicated to providing users with academic-level answers to questions about history, provides an interesting case study on the impact of design and culture because of its unique rules and their strict enforcement by moderators. In this article I use interviews with r/AskHistorians moderators and community members, observation, and the full comment log of a highly upvoted thread to describe the impact of Reddit's design and culture on moderation work. Results show that visible moderation work that is often interpreted as censorship, and the default masculine whiteness of Reddit create challenges for moderators who use the subreddit as a public history site. Nonetheless, r/AskHistorians moderators have carved a space on Reddit where, through their public scholarship work, the community serves as a model for combating misinformation by building trust in academic processes.}
}

@article{halfaker_rise_2013,
  title = {The {{Rise}} and {{Decline}} of an {{Open Collaboration System}}},
  author = {Halfaker, Aaron and Geiger, R. Stuart and Morgan, Jonathan T. and Riedl, John},
  year = {2013},
  month = may,
  journal = {American Behavioral Scientist},
  volume = {57},
  number = {5},
  abstract = {Open collaboration systems, such as Wikipedia, need to maintain a pool of volunteer contributors to remain relevant. Wikipedia was created through a tremendous number of contributions by millions of contributors. However, recent research has shown that the number of active contributors in Wikipedia has been declining steadily for years and suggests that a sharp decline in the retention of newcomers is the cause. This article presents data that show how several changes the Wikipedia community made to manage quality and consistency in the face of a massive growth in participation have ironically crippled the very growth they were designed to manage. Specifically, the restrictiveness of the encyclopedia's primary quality control mechanism and the algorithmic tools used to reject contributions are implicated as key causes of decreased newcomer retention. Furthermore, the community's formal mechanisms for norm articulation are shown to have calcified against changes\textemdash especially changes proposed by newer editors.},
  langid = {english}
}

@inproceedings{jurgens_just_2019,
  title = {A {{Just}} and {{Comprehensive Strategy}} for {{Using NLP}} to {{Address Online Abuse}}},
  booktitle = {Proceedings of {{ACL}}},
  author = {Jurgens, David and Hemphill, Libby and Chandrasekharan, Eshwar},
  year = {2019},
  month = jul,
  urldate = {2019-08-14},
  abstract = {Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.}
}

@inproceedings{lampe_slashdot_2004,
  title = {Slash(Dot) and Burn: Distributed Moderation in a Large Online Conversation Space},
  booktitle = {Proceedings of {{CHI}}},
  author = {Lampe, Cliff and Resnick, Paul},
  year = {2004},
  urldate = {2018-11-05},
  abstract = {Can a system of distributed moderation quickly and consistently separate high and low quality comments in an online conversation? Analysis of the site Slashdot.org suggests that the answer is a qualified yes, but that important challenges remain for designers of such systems. Thousands of users act as moderators. Final scores for comments are reasonably dispersed and the community generally agrees that moderations are fair. On the other hand, much of a conversation can pass before the best and worst comments are identified. Of those moderations that were judged unfair, only about half were subsequently counterbalanced by a moderation in the other direction. And comments with low scores, not at top-level, or posted late in a conversation were more likely to be overlooked by moderators.},
  langid = {english}
}

@phdthesis{lo_when_2018,
  type = {Thesis},
  title = {When All You Have Is a Banhammer : The Social and Communicative Work of {{Volunteer}} Moderators},
  author = {Lo, Claudia (Claudia Wai Yu)},
  year = {2018},
  urldate = {2021-10-25},
  abstract = {The popular understanding of moderation online is that moderation is inherently reactive, where moderators see and then react to content generated by users, typically by removing it; in order to understand the work already being performed by moderators, we need to expand our understanding of what that work entails. Drawing upon interviews, participant observation, and my own experiences as a volunteer community moderator on Reddit, I propose that a significant portion of work performed by volunteer moderators is social and communicative in nature. Even the chosen case studies of large-scale esports events on Twitch, where the most visible and intense tasks given to volunteer moderators consists of reacting and removing user-generated chat messages, exposes faults in the reactive model of moderation. A better appreciation of the full scope of moderation work will be vital in guiding future research, design, and development efforts in this field.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2018-09-17T15:49:18Z}
}

@article{schluger_proactive_2022,
  title = {Proactive {{Moderation}} of {{Online Discussions}}: {{Existing Practices}} and the {{Potential}} for {{Algorithmic Support}}},
  author = {Schluger, Charlotte and Chang, Jonathan P. and {Danescu-Niculescu-Mizil}, Cristian and Levy, Karen},
  year = {2022},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW2},
  urldate = {2022-11-23},
  abstract = {To address the widespread problem of uncivil behavior, many online discussion platforms employ human moderators to take action against objectionable content, such as removing it or placing sanctions on its authors. Thisreactive paradigm of taking action against already-posted antisocial content is currently the most common form of moderation, and has accordingly underpinned many recent efforts at introducing automation into the moderation process. Comparatively less work has been done to understand other moderation paradigms---such as proactively discouraging the emergence of antisocial behavior rather than reacting to it---and the role algorithmic support can play in these paradigms. In this work, we investigate such a proactive framework for moderation in a case study of a collaborative setting: Wikipedia Talk Pages. We employ a mixed methods approach, combining qualitative and design components for a holistic analysis. Through interviews with moderators, we find that despite a lack of technical and social support, moderators already engage in a number of proactive moderation behaviors, such as preemptively intervening in conversations to keep them on track. Further, we explore how automation could assist with this existing proactive moderation workflow by building a prototype tool, presenting it to moderators, and examining how the assistance it provides might fit into their workflow. The resulting feedback uncovers both strengths and drawbacks of the prototype tool and suggests concrete steps towards further developing such assisting technology so it can most effectively support moderators in their existing proactive moderation workflow.}
}

@article{seering_metaphors_2020,
  title = {Metaphors in Moderation},
  author = {Seering, Joseph and Kaufman, Geoff and Chancellor, Stevie},
  year = {2020},
  month = oct,
  journal = {New Media \& Society},
  publisher = {{SAGE Publications}},
  urldate = {2021-10-15},
  abstract = {Volunteer content moderators are essential to the social media ecosystem through the roles they play in managing and supporting online social spaces. Recent work has described moderation primarily as a functional process of actions that moderators take, such as making rules, removing content, and banning users. However, the nuanced ways in which volunteer moderators envision their roles within their communities remain understudied. Informed by insights gained from 79 interviews with volunteer moderators from three platforms, we present a conceptual map of the territory of social roles in volunteer moderation, which identifies five categories with 22 metaphorical variants that reveal moderators' implicit values and the heuristics that help them make decisions. These metaphors more clearly enunciate the roles volunteer moderators play in the broader social media content moderation apparatus and can drive purposeful engagement with volunteer moderators to better support the ways they guide and shape their communities.},
  langid = {english}
}

@inproceedings{seering_reconsidering_2020,
  title = {Reconsidering {{Self-Moderation}}: The {{Role}} of {{Research}} in {{Supporting Community-Based Models}} for {{Online Content Moderation}}},
  booktitle = {Proceedings of {{CSCW}}},
  author = {Seering, Joseph},
  year = {2020},
  month = oct,
  urldate = {2020-10-19},
  abstract = {Research in online content moderation has a long history of exploring different forms that moderation can take, including both user-driven moderation models on community-based platforms like Wikipedia, Facebook Groups, and Reddit, and centralized corporate moderation models on platforms like Twitter and Instagram. In this work I review different approaches to moderation research with the goal of providing a roadmap for researchers studying community self-moderation. I contrast community-based moderation research with platforms and policies-focused moderation research, and argue that the former has an important role to play in shaping discussions about the future of online moderation. I provide six guiding questions for future research that, if answered, can support the development of a form of user-driven moderation that is widely implementable across a variety of social spaces online, offering an alternative to the corporate moderation models that dominate public debate and discussion.}
}

@inproceedings{seering_shaping_2017,
  title = {Shaping {{Pro}} and {{Anti-Social Behavior}} on {{Twitch Through Moderation}} and {{Example-Setting}}},
  booktitle = {Proceedings of {{CSCW}}},
  author = {Seering, Joseph and Kraut, Robert and Dabbish, Laura},
  year = {2017},
  urldate = {2018-10-02},
  abstract = {Online communities have the potential to be supportive, cruel, or anywhere in between. The development of positive norms for interaction can help users build bonds, grow, and learn. Using millions of messages sent in Twitch chatrooms, we explore the effectiveness of methods for encouraging and discouraging specific behaviors, including taking advantage of imitation effects through setting positive examples and using moderation tools to discourage antisocial behaviors. Consistent with aspects of imitation theory and deterrence theory, users imitated examples of behavior that they saw, and more so for behaviors from high status users. Proactive moderation tools, such as chat modes which restricted the ability to post certain content, proved effective at discouraging spam behaviors, while reactive bans were able to discourage a wider variety of behaviors. This work considers the intersection of tools, authority, and types of behaviors, offering a new frame through which to consider the development of moderation strategies.}
}

@inproceedings{zhang_conversations_2018,
  title = {Conversations {{Gone Awry}}: {{Detecting Early Signs}} of {{Conversational Failure}}},
  booktitle = {Proceedings of {{ACL}}},
  author = {Zhang, Justine and Chang, Jonathan P. and {Danescu-Niculescu-Mizil}, Cristian and Dixon, Lucas and Thain, Nithum and Hua, Yiqing and Taraborelli, Dario},
  year = {2018},
  abstract = {One of the main challenges online social systems face is the prevalence of antisocial behavior, such as harassment and personal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesirable behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged.},
  langid = {english}
}
