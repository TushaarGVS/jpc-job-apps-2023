\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{cite}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References (works to which I contrubted have my name \underline{underlined})}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\renewcommand{\section}[1]{\vspace{0.25\baselineskip}\noindent\textbf{#1.}}
\renewcommand{\subsection}[1]{\vspace{0.25\baselineskip}\noindent\textit{#1.}}

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

From their inception, online platforms have promised to make us more connected, acting as social spaces that could promote communication and collaboration. 
As these platforms have grown, however, this optimistic vision has given way to the cold reality that they often instead play host to \emph{antisocial} behaviors, including incivility, bullying, harassment, and hate speech. 
In response, platforms---and the communities they host---commonly engage in \emph{content moderation} practices designed to identify, remove, and/or prevent instances of antisocial behavior.
These practices range from \emph{reactively} removing or otherwise correcting antisocial behavior after it occurs, to \emph{proactively} encouraging prosocial behavior and preventing incidents of antisocial behavior from occurring in the first place.
While there has been growing interest in exploring how computational tools could aid content moderation, existing work has tended to cater purely towards reactive, centralized models of moderation---for instance, by developing algorithmic systems that can automatically detect possible instances of antisocial behavior so that they can be flagged for human review or even automatically removed altogether.

My research, by contrast, seeks to fill a currently underexplored gap: exploring how computational tools could fit in to more community-driven, proactive moderation workflows.
In pursuit of this goal, I devlop a combined social and technical research agenda, which involves engaging collaboratively with real online communities to identify their needs and the gaps in their computational toolkits, and applying those insights to guide the development of algorithmic methods that might address those needs.
Executing this research plan has resulted both in new insights into how communities approach content moderation, and in novel algorithmic methods for identifying conversations that, while not yet exhibting outright antisocial behavior, are nonetheless at risk of derailing into such undesirable outcomes.
In keeping with the overarching goal of meeting the needs of real communities, I have further developed these algorithmic methods into usable software tools, and collaborated with online communites to evaluate how well these tools address their needs.

\section{Identifying Community Moderation Practices and the Needs That Arise}
%The popular discourse around content moderation has tended to focus on the practices of large social media companies like Facebook and Twitter, which employ a centralized, platform-driven model of moderation: moderators employed or contracted by the company enforcing often-opaque rules from the top down, most often by reactively removing content that is considered to be in violation of those rules.
%Yet as a growing body of scholarship within social science and human-computer interaction has shown, moderation actually encompasses a much richer body of practices which extend all the way down to the community level \cite{brewer_inclusion_2020,lampe_slashdot_2004,seering_reconsidering_2020}.
%On the opposite extreme from the platform-driven moderation of large social media platforms lies community-driven moderation, embraced by smaller, interest-specific communities such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit \cite{chandrasekharan_internets_2018,halfaker_rise_2013}.
%Under this paradigm, moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.
Much prior work on the role of computational assistance in content moderation has tended to cater towards the moderation practices of large platforms, who employ a centralized, platform-driven model of moderation where company-employed moderators reactiely remove content that is deemed to be in violation of the platform's (often opaque) rules.
But a growing body of scholarship within social science and human-computer interaction has argued that this approach is ill-suited to the needs of smaller, more interest-driven online communities (such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit) \cite{jurgens_just_2019,seering_reconsidering_2020}.
These communities tend to instead embrace a community-driven model of moderation, wherein moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.
These volunteer moderators do not just reactively remove antisocial behavior after it happens---they also regularly engage with their communities in order to proactively steer their communities towards a more prosocial atmosphere, for instance by educating community members about the rules and norms and intervening in situations that they deem to be getting out of hand \cite{seering_shaping_2017,cai_what_2019,billings_understanding_2010}.
To make computationally assisted moderation useful to these communities, we must first examine the concrete steps volunteer moderators take in their proactie moderation, and from there identify specific aspects that could benefit from algorithmic assistance---problems that my work has tackled from both a qualtiative and quantitative perspective.

In a first step towards understanding volunteer moderators' work, I conducted a data-driven exploration of the Wikipedia editor community's relationship with its volunteer moderators and how this relationship factors in to moderation actions, which I presented at the 2019 Web Conference (WWW 2019).
This analysis focuses on Wikipedia's ``talk pages'', a pseudo-forum where Wikipedia editors discuss and coordinate edits.
Wikipedia prohibits a number of antisocial behaviors, such as personal attacks, in talk page discussions, and this policy is enforced by volunteer moderators known as administrators.
When an editor breaches this policy, a commonly used enforcement tool is to temporarily block them from editing Wikipedia and/or participating in talk page discussions.
Due to Wikipedia's ethos of open access, nearly all such moderation actions are publicly logged, and I leveraged these logs to examine the interactions between moderators and the editors who they temporarily blocked.

My analysis reveals that \emph{trust} is a key ingredient in community-driven moderation.
Editors who explicitly signal their acceptance of the moderator's judgement---as measured by the use of apologetic language that suggests acknolwedgment of wrongdoing---are 9\% less likely to get blocked again in the future; in other words, they become more willing to adhere to community norms.
By contrast, editors who signaled their rejection of the moderator's judgement---as measured by the use of accusatory language such as direct questioning and claims of unfairness directed towards the moderator---are 5-6\% \emph{more} likely to get blocked again for a future offense.
This trust-signaling can go both ways: Wikipedia moderators have the ability to shorten the duration of a block that they had previously given out, which can be interpreted as a form of clemency that signals a degree of trust in the blocked editor.
My analysis reveals that editors who receive such clemency are 7\% less likely to get blocked again in the future.

These results point to an important aspect of community-driven moderation: it is very much a \emph{collaborative} project, one in which regular community members are not merely passive observers but instead play a direct role.
Regular community members actively choose to accept---or not---the authority of volunteer moderators and the norms they promote, and may directly communicate their (un)acceptance to the moderators themselves.
Volunteer moderators, for their part, seek to bolster their relationship of mutual trust with the community; in doing so, they may make community members more willing to adhere to rules and norms, thereby proactively reducing the likelihood of antisocial behavior.
But how is this concretely achieved?
My preceding data-driven analysis only reveals one strategy, the granting of clemency, which is applicable only in specific circumstances.
Gaining a more complete picture of proactive moderation work requires taking a more holistic approach to analyzing the work of volunteer moderators as seen directly from their own perspectives.

To gain such a perspective, I conducted interviews with Wikipedia moderators, in work published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
These interviews focused on the proactive side of volunteer moderators' work, asking questions about moderators reason about the likelihood of a stiuation deteriorating into antisocial behavior, and what kinds of proactive steps they take to prevent such outcomes.
From the responses, a clear picture emerges of how volunteer moderators can tell when conflicts between Wikipedia editors are at risk of veering away from healthy disagreement and towards antisocial behavior, and the strategies they employ to get things back on track.

All moderators we interviewed agreed that they can at least sometimes foresee when a discussion is likely to derail into antisocial behavior.
While exactly how they do this is more of an art than a science, moderators were able to point to some concrete indicators that they intuitively know to look for, such as the tone of the discussion and whether it has ``started off on the wrong foot''.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
