\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\renewcommand{\section}[1]{\vspace{0.25\baselineskip}\noindent\textbf{#1.}}
\renewcommand{\subsection}[1]{\vspace{0.25\baselineskip}\noindent\textit{#1.}}

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

From their inception, online platforms have promised to make us more connected, acting as social spaces that could promote communication and collaboration. 
As these platforms have grown, however, this optimistic vision has given way to the cold reality that they often instead play host to \emph{antisocial} behaviors, including incivility, bullying, harassment, and hate speech. 
In response, platforms---and the communities they host---commonly engage in \emph{content moderation} practices designed to identify, remove, and/or prevent instances of antisocial behavior.
These practices range from \emph{reactively} removing or otherwise correcting antisocial behavior after it occurs, to \emph{proactively} encouraging prosocial behavior and preventing incidents of antisocial behavior from occurring in the first place.
While there has been growing interest in exploring how computational tools could aid content moderation, existing work has tended to cater purely towards reactive, centralized models of moderation---for instance, by developing algorithmic systems that can automatically detect possible instances of antisocial behavior so that they can be flagged for human review or even automatically removed altogether.

My research, by contrast, seeks to fill a currently underexplored gap: exploring how computational tools could fit in to more community-driven, proactive moderation workflows.
In pursuit of this goal, I devlop a combined social and technical research agenda, which involves engaging collaboratively with real online communities to identify their needs and the gaps in their computational toolkits, and applying those insights to guide the development of algorithmic methods that might address those needs.
Executing this research plan has resulted both in new insights into how communities approach content moderation, and in novel algorithmic methods for identifying conversations that, while not yet exhibting outright antisocial behavior, are nonetheless at risk of derailing into such undesirable outcomes.
In keeping with the overarching goal of meeting the needs of real communities, I have further developed these algorithmic methods into usable software tools, and collaborated with online communites to evaluate how well these tools address their needs.

\section{Identifying Community Moderation Practices and the Needs That Arise}
The popular discourse around content moderation has tended to focus on the practices of large social media companies like Facebook and Twitter, which employ a centralized, platform-driven model of moderation: moderators employed or contracted by the company enforcing often-opaque rules from the top down.
Yet as a growing body of scholarship within social science and human-computer interaction has shown, moderation actually encompasses a much richer body of practices which extend all the way down to the community level \cite{brewer_inclusion_2020,lampe_slashdot_2004,seering_reconsidering_2020}.
On the opposite extreme from the platform-driven moderation of large social media platforms lies community-driven moderation, embraced by smaller, interest-specific communities such as Discord servers, Twitch livestream communities, topic-focused ``subreddits'' on Reddit, and the Wikipedia editor community \cite{chandrasekharan_internets_2018,halfaker_rise_2013,lampe_slashdot_2004}.
Under this paradigm, moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
