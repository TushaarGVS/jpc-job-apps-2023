\documentclass[12pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{cite}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References (works to which I contrubted have my name \underline{underlined})}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\titleformat{\section}{\normalfont\normalsize\bfseries\scshape}{}{0em}{}
\titlespacing*{\section}{0em}{0.5\baselineskip}{0em}
\titleformat{\subsection}[runin]{\normalfont\normalsize\bfseries}{}{0em}{}[.]
\titlespacing*{\subsection}{0em}{0.25\baselineskip}{0.5em}


%% LOGIC FOR CUSTOMIZING THE STATEMENT FOR INDIVIDUAL SCHOOLS
% set up common defines (commands and boolean flags)
\input{../defines.tex}
% import the school-specific header to make the magic happen!
\input{schoolheader.tex}
%% END CUSTOMIZATION LOGIC

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

One of the biggest problems facing online platforms today is the prevalence of incivility, harassment, hate speech, and other similar behaviors---collectively referred to in social science as ``antisocial behavior''.
While a lot of Computer Science research has responded to this problem by developing algorithms to detect antisocial behavior, I argue that this approach implicitly centers the perspective of platform owners, who would be in a position to employ these algorithms for moderation, and fails to center an equally important perspective: that of the \emph{communities} of ordinary people who interact on these platforms.
Therefore, my research focuses on the following question: how can technology help online communities to maintain norms and negotiate disputes---in other words, to \emph{proactively} prevent antisocial behavior from taking root?
I tackle this question through a combined social and technical research agenda, in which I have engaged with real online communities to gain novel insights about their needs, pioneered new algorithmic methods to address those needs, and followed up with those communities to collaboratively evaluate my new algorithmic methods and identify future directions for research.
\ifliberalarts
This research agenda is particularly well suited to a liberal arts institution like \schoolname, giving undergraduate students an opportunity to work with cutting-edge technology and simultaneously study the impact of this technology on society.
\else
%
\fi

\section{Background: The Landscape of Moderation}

%Popular discourse on content moderation tends to focus on the practices of social media companies, who often employ moderators to remove content deemed to be in violation of their (often opaque) rules.
%Yet a growing body of scholarship in social science and human-computer interaction argues that moderation actually encompasses a much richer body of practices extending all the way down to the community level \cite{brewer_inclusion_2020,seering_reconsidering_2020}.
While moderation is commonly thought of as the purview of social media companies, who often employ moderators to remove content deemed to be in violation of their rules, social and political scientists have long recognized that many online communities also self-organize some less formal moderation practices \cite{grimmelmann_virtues_2015}.
This community-driven moderation is often organized by volunteers, who play a role akin to a community leader \cite{seering_reconsidering_2020}.
But these volunteer moderators do not act alone: they also involve ordinary community members in the moderation process, by engaging with the community to promote rule-following behavior through steps like publicly modeling good behavior, educating community members about the rules, and mediating discussions that are getting out of hand \cite{seering_shaping_2017,billings_understanding_2010}.
These practices can be seen as \emph{proactive} moderation aimed at discouraging antisocial behavior from occurring at all---in contrast to \emph{reactive} moderation practices like content removal which respond to antisocial behavior after it occurs \cite{lo_when_2018}.



%Leveraging these logs, I conducted a data-driven analysis of how moderators interact with the editors whom they temporarily block, presented at the 2019 Web Conference (WWW 2019) \cite{chang_trajectories_2019}.
%A key finding was the importance of two-way trust in this relationship: blocked editors can signal trust in the moderator by acknowledging their wrongdoing (as measured through handwritten rules that detect actions such as apologies) while moderators can signal trust in the editor by granting requests for reduced block duration.
%Both such strategies are correlated with an increased likelihood of future rule-following by the blocked editor, showing how building trust may be a key factor in proactively fostering a prosocial, norm-adhering culture within an online community.


%These results point to an important aspect of community-driven moderation: it is very much a \emph{collaborative} project, one in which regular community members are not merely passive observers but instead play a direct role.
%Regular community members actively choose to accept---or not---the authority of volunteer moderators and the norms they promote, and may directly communicate their (un)acceptance to the moderators themselves.
%Volunteer moderators, for their part, seek to bolster their relationship of mutual trust with the community; in doing so, they may make community members more willing to adhere to rules and norms, thereby proactively reducing the likelihood of antisocial behavior.
%But how is this concretely achieved?
%My preceding data-driven analysis only reveals one strategy, the granting of clemency, which is applicable only in specific circumstances.
%Gaining a more complete picture of proactive moderation work requires taking a more holistic approach to analyzing the work of volunteer moderators as seen directly from their own perspectives.

%These results illuminate the collaborative nature of community-driven moderation: regular community members are a key contributor to the functioning of the system, as its success requires their buy-in and trust, which moderators in turn must foster.
%But besides grating clemency---which is a highly situational, niche strategy---how do volunteer moderators bolster their relationship with their community and proactively encourage prosocial behavior?
%To answer this question, I conducted interviews with Wikipedia moderators, in work published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%These interviews focused on the proactive side of volunteer moderators' work, asking questions about moderators reason about the likelihood of a stiuation deteriorating into antisocial behavior, and what kinds of proactive steps they take to prevent such outcomes.
%From the responses, a clear picture emerges of how volunteer moderators can tell when conflicts between Wikipedia editors are at risk of veering away from healthy disagreement and towards antisocial behavior, and the strategies they employ to get things back on track.

%To gain a broader view of proactive moderation strategies, I turned to interviews with Wikipedia moderators, which formed the basis of work presented at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%All moderators in the interviews reported that they have some intuition for when a discussion is at risk of derailing into personal attacks or other prohibited antisocial behaviors.
%Furthermore, they described how when they discover such at-risk discussions, they may intervene to gently steer the conversation back on track, through techniques such as offering to mediate the disagreement.

\section{My Research Contributions}
My research aims to fill a gap in the existing Computer Science research on content moderation: while there has been a lot of work on computational tools to assist reactive moderation, such as algorithms to detect antisocial content and flag it for removal, I focus instead on the relatively understudied question of how computational methods can assist community-driven proactive moderation.

\subsection{Identifying Community Moderators' Needs}
When developing technology meant to help a particular group, it is good practice to hear the perspective of people from that group and let them explain their needs in their own words.
Following this practice, I conducted interviews with volunteer moderators, with an emphasis on understanding how they reason about mediating discussions that are getting out of hand; this work was done alongside undergraduate student Charlotte Schluger and presented at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022; a top venue for research on online communities) \cite{schluger_proactive_2022}.
These interviews revealed that moderators can often intuitively tell if a discussion is at risk of later derailing into antisocial behavior and therefore in need of intervention---that is, they are able to \emph{forecast} derailment---but feel challenged by the sheer scale of their communities, often missing opportunities to intervene because there are too many discussions to keep track of.
They subsequently expressed a desire for effective computational tools to aid in this process.
We posit that meeting this need requires an algorithm that can forecast derailment much like human moderators do; such an algorithm could supplement existing community-driven moderation practices, such as by automatically serving reminders of the rules (mimicking a strategy used by human moderators) when an at-risk discussion is detected.


\subsection{Demonstrating the Feasibility of Algorithmically Forecasting Derailment}
To demonstrate that forecasting conversational derailment is a feasible task for computers, I developed two experiments which respectively studied algorithmic estimates of impoliteness and likelihood of misperception, both of which are factors theorized by socio-linguistics literature to be associated with tension and conflict.
The experiments, conducted in collaboration with industry partners, found that when these factors were present early in a discussion, as measured by machine learning models, the discussion was at statistically significantly higher risk of future derailment.
These results were respectively presented at 2018 Annual Meeting of the Association for Computational Linguistics (ACL 2018) \cite{zhang_conversations_2018} and the 2020 Web Conference (WWW 2020) \cite{chang_dont_2020}, both top venues for research applying natural language processing to computational social science.

\subsection{Practical Forecasting of Conversational Derailment}
Classical machine learning approaches to natural language processing 
%(like those used in my earlier proof-of-concept work) 
typically operate on a fixed snapshot of a discussion, assuming it will never change.
But such an approach is impractical for forecasting derailment in real moderation settings, because online discussions actually evolve over time, and each new comment might increase or decrease the risk of derailment.
Thus, a practically useful algorithm would need to follow a conversation in real time, and at each new comment update its belief about whether the conversation is at risk of derailment.
In work presented at the ACL's 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019; a hub for debuting novel natural language models) \cite{chang_trouble_2019}, I was able to develop such an algorithm, a neural network known as CRAFT, by adapting techniques from the domain of dialog modeling (aka ``chatbots'') \cite{serban_building_2016}, where similar practical challenges are present.
Experiments on conversations from real online communities show that 77\% of discussions that actually derail are correctly detected by CRAFT (a standard machine learning evaluation metric known as recall); conversely, across all conversations where CRAFT predicts derailment, 64\% in fact go on to derail (a standard metric known as precision).

\subsection{Using Forecasting Algorithms to Support Online Communities}
While CRAFT scores highly on standard machine learning metrics, these metrics do not necessarily translate into actual usefulness for humans.
Evaluating whether a forecasting algorithm like CRAFT can actually benefit real online communities required joining the social and technical aspects of my work.
On the technical side, I led a team of undergraduate and masters students to develop a prototype tool, ConvoWizard, that uses CRAFT to implement one of the ideas discussed in the moderator interviews: automatically warning users when a discussion they are replying to may be at risk of derailing into antisocial behavior.
On the social side, I arranged a groundbreaking collaboration with several real online communities to conduct a real-world test run of ConvoWizard in a way that was respectful of the community's norms.

This work is still in progress, but preliminary results published in CSCW 2022 \cite{chang_thread_2022} offer promising signs of ConvoWizard's potential to improve online discussions.
Upon seeing warnings from ConvoWizard, users tended to make edits that decrease the risk of derailment (as estimated by CRAFT), unlike in a control condition where edits tended to increase the risk.
A linguistic analysis of the resulting comments showed that they tend to exhibit increased use of known conflict avoidance strategies such as asking questions and adopting more formal tone.
Finally, in an exit survey conducted after the test run, ConvoWizard users left largely positive evaluations of the tool: most notably, 54\% reported that ConvoWizard made them rethink posting a comment they might have later regretted, 83\% expressed interest in long-term use of a ConvoWizard-style tool if it became publicly available, and 64\% felt that if ConvoWizard were deployed at scale, the net effect would be an improvement in discussion quality.

\section{Future Directions}
I am a firm believer in the principle that before new technologies can be broadly adopted, we must first try to understand as fully as possible its ramifications for society and its potential risks.
This is especially vital for work relating to content moderation, which is quickly becoming a major societal issue with implications for online safety, free speech, and social justice.
In line with this principle, the social aspect of my prior and ongoing work has been tailored towards understanding the impact of tools based on forecasting algorithms, so that any future large-scale deployment of such technology can be done in a responsible and ethical manner.
While the findings thus far have offered positive signs about the potential of this technology to improve online discussions, they also raise important new questions about the limitations of the technology and what can be done to improve it.

\subsection{Improving the Transparency of Forecasting Algorithms}
The most common complaint we received in the ConvoWizard exit survey was that the tool lacked \emph{transparency}: when the tool reports that a discussion might be at risk of derailment, it does not explain why this might be the case.
Users reported that this sometimes made it difficult to decide how they should respond, especially in cases where they disagreed with ConvoWizard's judgment.
Solving this problem will likely form the central focus of my research in the immediate future, and to this end I have already begun some preliminary work to explore possible directions.
During this past summer (Summer 2023), I worked with two undergraduate students to design new studies that will investigate how humans tend to judge the risk of derailment and what they would find helpful in an explanation of this risk.
I plan to use the insights from these studies, in combination with the latest technical innovations in explainable AI and text generation, to guide the development of next-generation conversational forecasting algorithms that can automatically generate human-readable explanations of their forecasts.

%But the transparency issue is not just a technical problem: it also ties in to the larger \emph{social} problem of algorithmic bias.
%By now, it has become well known that data-driven approaches are vulnerable to capturing biases embedded within the data.
%The question, then, is not whether forecasting algorithms like CRAFT contain biases---the answer is surely yes---but rather what kinds of biases exist and what their impact would be in a broadly deployed forecasting-based tool.
%This is uncharted territory, since most prior work on computational tools for moderation---and by extension, the analyses of their biases---focused on the reactive paradigm, and it is not clear how those findings might apply to proactive moderation.
%Like the rest of my research agenda, my plan for addressing this question is to take a combined social and technical approach.
%Once an explainable successor to CRAFT has been developed, my plan is to analyze the model's decisions through the lens of recent work from computational social science on codifying the social and power implications found in natural language \cite{sap_social_2020}.
%Cross-referencing this work with the model's decision-making processes could reveal, for instance, whether the algorithm is less sensitive to attacks towards specific underrepresented groups.
%And once such biases are identified, the same insights from social science could be applied to design ways to counter them.

\subsection{Incorporating Non-linguistic Context}
The models I have developed thus far look only at the linguistic content of the conversation itself to forecast whether the conversation will derail.
In practice, however, the trajectory of a conversation often depends on factors beyond what is directly said in the comments; for example, prior work has shown that a community member's willingness to adhere to community norms is influenced by how long they have been part of the community \cite{danescu-niculescu-mizil_no_2013}.
I am interested in investigating how recent breakthroughs in so-called multimodal models---that is, models capable of combining language data with other modes of input---could be adapted to enable forecasting algorithms to incorporate this non-linguistic context and potentially make more accurate predictions.



\subsection{Measuring Longer-term Impacts of Proactive Moderation Tools}
Evaluations of ConvoWizard thus far have focused on the tool's immediate impact: that is, how seeing a warning when replying to an at-risk discussion impacts the user's behavior while writing the reply.


It is also important once again to understand the impact of tools based on forecasting algorithms not just in the lab, but in the context of real communities.
All the future plans I just described should therefore be accompanied by further collaborative studies with real communities, including much larger-scale studies aimed at understanding long-term impacts, not just immediate implications on individual discussion threads.
As described earlier, these studies will also look into how forecasting algorithms might be used to directly support moderators, in additional to ConvoWizard-style tools aimed at regular community members.

\section{Involving Undergraduates in my Research Agenda}
Throughout my research, I have had the pleasure and privilege of collaborating with several undergraduate students, who have contributed to my research agenda in a number of ways.
Most notably, two of my papers---the two CSCW papers---were co-authored with undergraduate student Charlotte Schluger, who is in fact first author on one of them.
She had the distinction of contributing through the entire research pipeline: she was part of the larger student team that implemented the ConvoWizard research tool, was responsible for both designing and conducting the moderator interviews, and finally contributed large portions of the paper writing.
To me, Charlotte's experience illustrates the broad potential of my research agenda for undergraduates: it presents opportunities for both scientific contributions and software engineering contributions, thus catering to undergraduate students on any career path; indeed, other members of the ConvoWizard development team, knowing they wanted an industry job, chose to focus solely on the software engineering aspects.

My research agenda also opens up other paths for undergraduates to contribute beyond just work directly aimed at publications.
I have a strong commitment to open source and open science, and in this spirit I have been a core contributor to ConvoKit (\url{https://convokit.cornell.edu}), an open source Python package designed to empower research on conversational data.
As part of my work on ConvoKit I have mentored a number of undergraduate contributors, who again have gone onto a variety of career paths.
In fact, I met my first undergraduate mentee, Andrew Wang, through his work on ConvoKit, which he used as a launching point to get into research; he would later go on to receive a MS degree at Stanford followed by returning to Cornell to pursue a PhD.
On a different type of career path is Caleb Chiam, who over his 3 years working with us as an undergraduate became one of the biggest contributors to ConvoKit---experience which now serves him well at his current software engineering career.

Overall, I believe that my research agenda is a perfect fit for an undergraduate teaching institution.
Because of my focus on practical use cases, my research involves a heavy software engineering focus alongside the typical scientific aspects.
As my prior experiences with undergraduate students have shown, this means that my research has something to offer for everyone---a particularly important feature at an undergraduate institution, where students may be pursuing a variety of different post-graduation trajectories, or in fact trying to decide on one.
My continued commitment to open source and the ConvoKit project also offers a unique opportunity for students to have an impact on a real tool that is used by researchers and software engineers while simultaneously getting exposure to research.
In light of all of this, I feel that I can strongly contribute to the education of students at UNIVERSITY\_NAME.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
