\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{cite}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References (works to which I contrubted have my name \underline{underlined})}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\renewcommand{\section}[1]{\vspace{0.25\baselineskip}\noindent\textbf{#1.}}
\renewcommand{\subsection}[1]{\vspace{0.25\baselineskip}\noindent\textit{#1.}}

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

From their inception, online platforms have promised to make us more connected, acting as social spaces that could promote communication and collaboration. 
As these platforms have grown, however, this optimistic vision has given way to the cold reality that they often instead play host to \emph{antisocial} behaviors, including incivility, bullying, harassment, and hate speech. 
%In response, platforms---and the communities they host---commonly engage in \emph{content moderation} practices designed to identify, remove, and/or prevent instances of antisocial behavior.
%These practices range from \emph{reactively} removing or otherwise correcting antisocial behavior after it occurs, to \emph{proactively} encouraging prosocial behavior and preventing incidents of antisocial behavior from occurring in the first place.
%While there has been growing interest in exploring how computational tools could aid content moderation, existing work has tended to cater purely towards reactive, centralized models of moderation---for instance, by developing algorithmic systems that can automatically detect possible instances of antisocial behavior so that they can be flagged for human review or even automatically removed altogether.
%
%My research, by contrast, seeks to fill a currently underexplored gap: exploring how computational tools could fit in to more community-driven, proactive moderation workflows.
My research focuses on characterizing the social and linguistic contexts that precipitate such antisocial behavior, with the end goal of designing computational tools to empower community-driven, proactive moderation workflows---standing in contrast to prior work which has tended to instead focus on top-down, reactive moderation.
In pursuit of this goal, I devlop a combined social and technical research agenda, which involves engaging collaboratively with real online communities to identify their needs and the gaps in their computational toolkits, and applying those insights to guide the development of algorithmic methods that might address those needs.
%Executing this research plan has resulted both in new insights into how communities approach content moderation, and in novel algorithmic methods for identifying conversations that, while not yet exhibting outright antisocial behavior, are nonetheless at risk of derailing into such undesirable outcomes.
%In keeping with the overarching goal of meeting the needs of real communities, I have further developed these algorithmic methods into usable software tools, and collaborated with online communites to evaluate how well these tools address their needs.
This agenda has so far yielded new insights into proactive moderation practices, novel algorithmic approaches to characterizing when discussions are at risk of derailing into antisocial behavior, and user studies done in collaboration with real online communities to evaluate the practical utility of these algorithmic methods.

\section{Identifying Community Moderation Practices and the Needs That Arise}
%The popular discourse around content moderation has tended to focus on the practices of large social media companies like Facebook and Twitter, which employ a centralized, platform-driven model of moderation: moderators employed or contracted by the company enforcing often-opaque rules from the top down, most often by reactively removing content that is considered to be in violation of those rules.
%Yet as a growing body of scholarship within social science and human-computer interaction has shown, moderation actually encompasses a much richer body of practices which extend all the way down to the community level \cite{brewer_inclusion_2020,lampe_slashdot_2004,seering_reconsidering_2020}.
%On the opposite extreme from the platform-driven moderation of large social media platforms lies community-driven moderation, embraced by smaller, interest-specific communities such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit \cite{chandrasekharan_internets_2018,halfaker_rise_2013}.
%Under this paradigm, moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.
Prior work on the role of computational assistance in content moderation has tended to cater towards the moderation practices of large platforms, which employ a centralized, platform-driven model of moderation where company-employed moderators reactiely remove content that is deemed to be in violation of the platform's (often opaque) rules.
But a growing body of scholarship within social science and human-computer interaction has argued that this approach is ill-suited to the needs of smaller, more interest-driven online communities (such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit) \cite{jurgens_just_2019,seering_reconsidering_2020}.
These communities tend to instead embrace a community-driven model of moderation, wherein moderators are volunteers who are actual members of the communities they serve \cite{lo_when_2018}.
These volunteer moderators do not just reactively remove antisocial behavior after it happens---they also regularly engage with their communities to proactively steer their communities towards a more prosocial atmosphere .
Such proactive steps include publicly modeling good behavior, reminding community members about the rules, and mediating discussions that are getting out of hand \cite{seering_shaping_2017,cai_what_2019,billings_understanding_2010}.


%Leveraging these logs, I conducted a data-driven analysis of how moderators interact with the editors whom they temporarily block, presented at the 2019 Web Conference (WWW 2019) \cite{chang_trajectories_2019}.
%A key finding was the importance of two-way trust in this relationship: blocked editors can signal trust in the moderator by acknowledging their wrongdoing (as measured through handwritten rules that detect actions such as apologies) while moderators can signal trust in the editor by granting requests for reduced block duration.
%Both such strategies are correlated with an increased likelihood of future rule-following by the blocked editor, showing how building trust may be a key factor in proactively fostering a prosocial, norm-adhering culture within an online community.


%These results point to an important aspect of community-driven moderation: it is very much a \emph{collaborative} project, one in which regular community members are not merely passive observers but instead play a direct role.
%Regular community members actively choose to accept---or not---the authority of volunteer moderators and the norms they promote, and may directly communicate their (un)acceptance to the moderators themselves.
%Volunteer moderators, for their part, seek to bolster their relationship of mutual trust with the community; in doing so, they may make community members more willing to adhere to rules and norms, thereby proactively reducing the likelihood of antisocial behavior.
%But how is this concretely achieved?
%My preceding data-driven analysis only reveals one strategy, the granting of clemency, which is applicable only in specific circumstances.
%Gaining a more complete picture of proactive moderation work requires taking a more holistic approach to analyzing the work of volunteer moderators as seen directly from their own perspectives.

%These results illuminate the collaborative nature of community-driven moderation: regular community members are a key contributor to the functioning of the system, as its success requires their buy-in and trust, which moderators in turn must foster.
%But besides grating clemency---which is a highly situational, niche strategy---how do volunteer moderators bolster their relationship with their community and proactively encourage prosocial behavior?
%To answer this question, I conducted interviews with Wikipedia moderators, in work published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%These interviews focused on the proactive side of volunteer moderators' work, asking questions about moderators reason about the likelihood of a stiuation deteriorating into antisocial behavior, and what kinds of proactive steps they take to prevent such outcomes.
%From the responses, a clear picture emerges of how volunteer moderators can tell when conflicts between Wikipedia editors are at risk of veering away from healthy disagreement and towards antisocial behavior, and the strategies they employ to get things back on track.

%To gain a broader view of proactive moderation strategies, I turned to interviews with Wikipedia moderators, which formed the basis of work presented at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%All moderators in the interviews reported that they have some intuition for when a discussion is at risk of derailing into personal attacks or other prohibited antisocial behaviors.
%Furthermore, they described how when they discover such at-risk discussions, they may intervene to gently steer the conversation back on track, through techniques such as offering to mediate the disagreement.

To gain an understanding of the needs that volunteer moderators face when doing their proactive work, I conducted interviews with moderators on Wikipedia's ``talk pages'' (a pseudo-forum where Wikipedia editors discuss and coordinate edits), resulting in findings that were published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
These findings illuminate a key challenge in proactive moderation: deciding where and when action is needed.
All moderators we interviewed told us they have some intuition about what makes a discussion at risk of derailing into future antisocial behavior, and that they use this intuition to determine when they might want to step in try to set the discussion back on track with soft, informal intervention strategies---for instance, reminding the discussion participants about talk page conduct policies, or offering to serve as a neutral third party mediator.
But this intuition is far from perfect, with all moderators simultaneously indicating some degree of uncertainty about their intuitive judgements.
This difficulty is compounded by the large volume of discussions taking place at any given time, making it impossible to keep an eye on every single discussion that could turn out to be at risk.
Finally, even when a moderator decides a discussion is at risk and requires intervention, knowing exactly \emph{when} to intervene is a challenge of its own---due to the inherent uncertainty of predicting risk of derailment, moderators often hesitate to intervene too early, but if they wait too long the discussion may reach a point where it becomes too late to set things back on track.%; as one moderator poetically put it ``the hand grenade has gone off and I didn't even hear it because I've gone down the street''.

I believe that these challenges represent an opening where computational tools could be of assistance to volunteer moderators.
If an algorithm could capture some of the human intuition about when a discussion is at risk of future derailment, it could be used to automatically identify conversations that might require intervention, taking over some of the burden of manually finding such cases.
Additionally, such an algorithm could benefit regular community members by offering guidance on how their comments might affect the discussion and helping them avoid escalating at-risk situations, much like moderators' own soft interventions do.
Actually accomplishing this requires both developing this hypothetical derailment-forecasting algorithm, and doing design work to adapt it into a practical tool that is useful and, more importantly, safe---challenges which I have already begun to address.

\section{Evaluating the Feasibility of Algorithmically Forecasting Derailment}
The first step towards practical computational tools for proactive moderation is to show that it is feasible in the first place for an algorithm to pick up on signals of potential future derailment, like human moderators claim to be able to do.
To establish the feasibility of this task, we must look back to some of my earlier work on analyzing conversations, which focused on finding experimental evidence to support sociolinguistic theories about how to mediate tense conversations.

Classical literature in the field of pragmatics points to \emph{politeness} as a mediating factor that softens the perceived force of a message \cite{brown_politeness:_1987,clark_polite_1980-1}.
In work presented at the 2018 Annual Meeting of the Association for Computational Linguistics (ACL 2018) \cite{zhang_conversations_2018}, my collaborators and I examined this theory computationally by building a software system that characterizes politeness by applying a combination of handwritten rules inspired by the classical theories \cite{danescu-niculescu-mizil_computational_2013} and an unsupervised method for estimating the rhetorical intent of an utterance \cite{zhang_asking_2017}.
We ran this system on comments sampled from Wikipedia talk page discussion, and compared the system-detected politeness features to the eventual outcome of the discussion (as labeled by human annotators).
Consistent with the classical theories, we confirmed that politness is correlated with reduced likelihood of antisocial outcomes.

More recent theoretical work has posited that misalignment between what a speaker \emph{intends} to communicate and how a listener \emph{perceives} them can spark conflict leading to negative outcomes \cite{tannen_indirectness_2000}.
Inspired by these theories, work conducted during an internship at Facebook and presented at the 2020 Web Conference (WWW 2020) \cite{chang_dont_2020}, my collaborators and I conducted a large-scale survey of Facebook users to obtain unprecedented ground-truth data about comment authors' intentions and readers' perceptions.
Through a regression analysis, we confirmed that discussions containing a misalignment between intentions and perceptions are more likely to end in antisocial behavior (as measured by an in-house moderation tool).

Together, these works represent an empirical validation of theories of communication, which support the idea that an algorithm for forecasting derailment in conversations is feasible.
However, neither work actually represents a practically usable algorithm for such forecasting: both employ simplified, proof-of-concept models that examine only isolated comments rather than following entire conversations in real time.
Achieving the latter introduces practical challenges that require a more sophisticated model, the focus of my subsequent work.

\section{Practical Forecasting of Conversational Derailment}
Going beyond the level of individual comments to algorithmically detect risk of derailment at the \emph{conversation} level introduces two key practical challenges.
First, conversations have \emph{unknown horizon}: rather than coming into existence fully-formed, online discussions grow and evolve over an extended period of time, which means a practical algorithm for forecasting derailment cannot just make a single prediction, but must instead follow a conversation as it develops in real time and update its predictions to reflect changes in the discussion.
Second, conversations are \emph{dynamic}: utterances in a discussion cannot be treated as isolated objects, because the meaning of an utterance often depends on context provided by previous utterances; a practical algorithm needs to be able to model such dependencies.

To address both challenges, I started with a key insight that these same challenges are faced by developers of dialog models (a.k.a. ``chatbots'')---like a forecasting algorithm, a dialog model also needs to follow a conversation in real time so that it can generate new replies on demand, and it needs to understand the relationships between utterances so it can generate situationally appropriate responses (e.g., knowing that some types of questions should be followed by an answer while other types are rhetorical).
Inspired by this insight, I adapted an existing neural network approach for dialog modeling \cite{serban_building_2016}: I first trained the network on the original dialog modeling task so it could learn conversational dynamics, then trained a classifier layer on top of the dialog model, which is used to produce a forecast (likelihood of derailment) after each new utterance.
The resulting model, which I have dubbed CRAFT, was presented at the ACL's 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019) \cite{chang_trouble_2019}.
Tested on the same Wikipedia talk page dataset as my earlier ACL 2018 paper, I showed that in 77\% of discussions that actually derail are correctly detected by CRAFT (the metric of recall); and conversely, across all conversations where CRAFT predicts derailment, 64\% in fact go on to derail (the metric of precision).
On average, CRAFT's first prediction of derailment in an actually-derailing conversation happens 3 comments before the outright antisocial comment (as identified by human annotators), giving a comfortable margin of advance warning.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
