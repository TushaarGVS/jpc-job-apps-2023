\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{cite}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References (works to which I contrubted have my name \underline{underlined})}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\renewcommand{\section}[1]{\vspace{0.25\baselineskip}\noindent\textbf{#1.}}
\renewcommand{\subsection}[1]{\vspace{0.25\baselineskip}\noindent\textit{#1.}}

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

From their inception, online platforms have promised to make us more connected, acting as social spaces that could promote communication and collaboration. 
As these platforms have grown, however, this optimistic vision has given way to the cold reality that they often instead play host to \emph{antisocial} behaviors, including incivility, bullying, harassment, and hate speech. 
%In response, platforms---and the communities they host---commonly engage in \emph{content moderation} practices designed to identify, remove, and/or prevent instances of antisocial behavior.
%These practices range from \emph{reactively} removing or otherwise correcting antisocial behavior after it occurs, to \emph{proactively} encouraging prosocial behavior and preventing incidents of antisocial behavior from occurring in the first place.
%While there has been growing interest in exploring how computational tools could aid content moderation, existing work has tended to cater purely towards reactive, centralized models of moderation---for instance, by developing algorithmic systems that can automatically detect possible instances of antisocial behavior so that they can be flagged for human review or even automatically removed altogether.
%
%My research, by contrast, seeks to fill a currently underexplored gap: exploring how computational tools could fit in to more community-driven, proactive moderation workflows.
My research focuses on characterizing the social and linguistic contexts that precipitate such antisocial behavior, with the end goal of designing computational tools to empower community-driven, proactive moderation workflows---standing in contrast to prior work which has tended to instead focus on top-down, reactive moderation.
In pursuit of this goal, I devlop a combined social and technical research agenda, which involves engaging collaboratively with real online communities to identify their needs and the gaps in their computational toolkits, and applying those insights to guide the development of algorithmic methods that might address those needs.
%Executing this research plan has resulted both in new insights into how communities approach content moderation, and in novel algorithmic methods for identifying conversations that, while not yet exhibting outright antisocial behavior, are nonetheless at risk of derailing into such undesirable outcomes.
%In keeping with the overarching goal of meeting the needs of real communities, I have further developed these algorithmic methods into usable software tools, and collaborated with online communites to evaluate how well these tools address their needs.
This agenda has so far yielded new insights into proactive moderation practices, novel algorithmic approaches to characterizing when discussions are at risk of derailing into antisocial behavior, and user studies done in collaboration with real communities to evaluate the practical utility of these algorithmic methods.

\section{Identifying Community Moderation Practices and the Needs That Arise}
%The popular discourse around content moderation has tended to focus on the practices of large social media companies like Facebook and Twitter, which employ a centralized, platform-driven model of moderation: moderators employed or contracted by the company enforcing often-opaque rules from the top down, most often by reactively removing content that is considered to be in violation of those rules.
%Yet as a growing body of scholarship within social science and human-computer interaction has shown, moderation actually encompasses a much richer body of practices which extend all the way down to the community level \cite{brewer_inclusion_2020,lampe_slashdot_2004,seering_reconsidering_2020}.
%On the opposite extreme from the platform-driven moderation of large social media platforms lies community-driven moderation, embraced by smaller, interest-specific communities such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit \cite{chandrasekharan_internets_2018,halfaker_rise_2013}.
%Under this paradigm, moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.
Prior work on the role of computational assistance in content moderation has tended to cater towards the moderation practices of large platforms, who employ a centralized, platform-driven model of moderation where company-employed moderators reactiely remove content that is deemed to be in violation of the platform's (often opaque) rules.
But a growing body of scholarship within social science and human-computer interaction has argued that this approach is ill-suited to the needs of smaller, more interest-driven online communities (such as the Wikipedia editor community and topic-focused ``subreddits'' on Reddit) \cite{jurgens_just_2019,seering_reconsidering_2020}.
These communities tend to instead embrace a community-driven model of moderation, wherein moderators are volunteers who are actual members of the communities they serve \cite{gilbert_i_2020,lo_when_2018}.
These volunteer moderators do not just reactively remove antisocial behavior after it happens---they also regularly engage with their communities to proactively steer their communities towards a more prosocial atmosphere \cite{seering_shaping_2017,cai_what_2019,billings_understanding_2010}.
To make computationally assisted moderation useful to these communities, we must first examine the concrete steps volunteer moderators take in their proactive moderation, and from there identify specific aspects that could benefit from algorithmic assistance.

My work has explored these questions through both quantitative and qualitative analysis of moderation actions on Wikipedia's ``talk pages''---a pseudo-forum where Wikipedia editors discuss and coordinate edits.
When editors violate talk page rules, such as a prohibition against personal attacks, volunteer moderators can take actions such as temporarily blocking them from editing Wikipedia and/or participating in talk page discussions.
Due to Wikipedia's ethos of open access, nearly all such moderation actions are publicly logged.

Leveraging these logs, I conducted a data-driven analysis of how moderators interact with the editors whom they temporarily block, presented at the 2019 Web Conference (WWW 2019).
A key finding was the importance of two-way trust in this relationship: blocked editors can signal trust in the moderator by acknowledging their wrongdoing (as measured through handwritten rules that detect actions such as apologies) while moderators can signal trust in the editor by granting requests for reduced block duration.
Both such strategies are correlated with an increased likelihood of future rule-following by the blocked editor, showing how building trust may be a key factor in proactively fostering a prosocial, norm-adhering culture within an online community.


%These results point to an important aspect of community-driven moderation: it is very much a \emph{collaborative} project, one in which regular community members are not merely passive observers but instead play a direct role.
%Regular community members actively choose to accept---or not---the authority of volunteer moderators and the norms they promote, and may directly communicate their (un)acceptance to the moderators themselves.
%Volunteer moderators, for their part, seek to bolster their relationship of mutual trust with the community; in doing so, they may make community members more willing to adhere to rules and norms, thereby proactively reducing the likelihood of antisocial behavior.
%But how is this concretely achieved?
%My preceding data-driven analysis only reveals one strategy, the granting of clemency, which is applicable only in specific circumstances.
%Gaining a more complete picture of proactive moderation work requires taking a more holistic approach to analyzing the work of volunteer moderators as seen directly from their own perspectives.

%These results illuminate the collaborative nature of community-driven moderation: regular community members are a key contributor to the functioning of the system, as its success requires their buy-in and trust, which moderators in turn must foster.
%But besides grating clemency---which is a highly situational, niche strategy---how do volunteer moderators bolster their relationship with their community and proactively encourage prosocial behavior?
%To answer this question, I conducted interviews with Wikipedia moderators, in work published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%These interviews focused on the proactive side of volunteer moderators' work, asking questions about moderators reason about the likelihood of a stiuation deteriorating into antisocial behavior, and what kinds of proactive steps they take to prevent such outcomes.
%From the responses, a clear picture emerges of how volunteer moderators can tell when conflicts between Wikipedia editors are at risk of veering away from healthy disagreement and towards antisocial behavior, and the strategies they employ to get things back on track.

To gain a broader view of proactive moderation strategies, I turned to interviews with Wikipedia moderators, which formed the basis of work presented at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022).
All moderators in the interviews reported that they have some intuition for when a discussion is at risk of derailing into personal attacks or other prohibited antisocial behaviors.
Furthermore, they described how when they discover such at-risk discussions, they may intervene to gently steer the conversation back on track, through techniques such as offering to mediate the disagreement.

Nonetheless, moderators also reported facing some key challenges in trying to proactively keep things civil.
For one, their own intuition about whether a conversation is at risk is far from infallible, with all moderators reporting at least some degree of uncertainty in their judgments.
This difficulty is compounded by the large volume of discussions taking place at any given time, making it impossible to keep an eye on every single discussion that could turn out to be at risk.
Finally, even when a moderator decides a discussion is at risk and requires intervention, knowing exactly \emph{when} to intervene is a challenge of its own---due to the inherent uncertainty of predicting risk of derailment, moderators often hesitate to intervene too early, but if they wait too long the discussion may reach a point where it becomes too late to set things back on track; as one moderator poetically put it ``the hand grenade has gone off and I didn't even hear it because I've gone down the street''.

I believe that these challenges represent an opening where computational tools could be of assistance to volunteer moderators.
If an algorithm could capture some of the human intuition about when a discussion is at risk of derailing, it could supplement volunteer moderators' work in at least two ways.
First, such an algorithm could form the basis of a moderator-facing tool that highlights, in real time, the discussions that are most at risk, taking over some of the burden of identifying where intervention is needed.
Second, such an algorithm could also provide feedback to regular community members, providing an automated version of moderators' interventions within at-risk discussions in order to encourage the participants to stay on track and avoid violating community norms.
Actually accomplishing this requires both developing this hypothetical derailment-forecasting algorithm, and doing design work to adapt it into a usable and useful tool---challenges which I have begun to address through my subsequent research.

\section{Evaluating the Feasibility of Algorithmically Forecasting Derailment}
The first step towards practical computational tools for proactive moderation is to show that it is feasible in the first place for an algorithm to pick up on signals of potential derailment, as human moderators claim to be able to do.
To establish the feasibility of this task, we must look back to some of my earlier work on analyzing conversations, which focused on finding experimental evidence to support sociolinguistic theories about how to mediate tense conversations.

One such work, presented at the 2018 Annual Meeting of the Association for Computational Linguistics (ACL 2018) \cite{zhang_conversations_2018}, examines the ability of \emph{politeness} to mediate discussions by acting as a face-saving measure and softening the force of a message, as posited by classic work in the field of pragmatics \cite{clark_polite_1980-1,brown_politeness:_1987}.
To examine this theory computationally, my collaborators and I built a software system that characterizes politeness by applying a combination of handwritten rules inspired by the classical theories \cite{danescu-niculescu-mizil_computational_2013} and a recently (at the time) developed unsupervised method for estimating the rhetorical intent of an utterance \cite{zhang_asking_2017}.
For evaluation, we constructed and released a dataset of Wikipedia talk page discussions labeled by human annotators as ending in personal attacks or remaining on track.
Applying our system to this data confirmed that politeness is predictive of discussions staying on track: using the politeness features as the basis for a simple linear classifier (a standard practice in classical natural language processing) yields a prediction accurary of 65\%, which approaches human accuracy of 72\% on the same task as measured through further human annotation.

A second theoretical indicator of derailment is \emph{misunderstanding}: prior work has posited that when there is a large gap between what a speaker \emph{intends} versus how they are \emph{perceived} by others in the conversation, this can hamper the discussion and result in negative outcomes \cite{tannen_indirectness_2000}.
However, empirically testing this hypothesis is challenging because reliable ground truth data is hard to obtain: only the speaker themself knows their true intention.
In groundbreaking work conducted during an internship at Facebook and presented at WWW 2020, my collaborators and I conducted the first large-scale empirical study of misperception in online conversations, obtaining ground truth data through a survey that asked authors of comments whether they intended to share a fact or an opinion, and authors of replies what they perceived the original comment to be doing.
Through a regression analysis, we confirmed that discussions containing a misperception between intentions and perceptions are, as predicted by theory, more likely to end in antisocial behavior (as measured by an in-house moderation tool).

Together, these works represent an empirical validation of theories of communication, which support the idea that an algorithm for forecasting derailment in conversations is feasible.
However, neither work actually represents a practically usable algorithm for such forecasting: both employ simplified, proof-of-concept models that examine only isolated comment-reply pairs rather than following entire conversations in real time.
Achieving the latter introduces practical challenges that require a more sophisticated model, the focus of my subsequent work.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
