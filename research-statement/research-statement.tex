\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
%\usepackage[bf,tiny,compact]{titlesec}
\usepackage{times}
\usepackage{cite}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{etoolbox}
\usepackage{hyperref}


\makeatletter
\def\@maketitle{%
    {\centering\fontsize{12pt}{14pt}\selectfont\textbf{\@title}\par}
}
\patchcmd{\@footnotetext}{\footnotesize}{\fontsize{10pt}{12pt}\selectfont}{}{}
\makeatother


\renewcommand{\bibsection}{\noindent\textbf{References (works to which I contrubted have my name \underline{underlined})}\vspace{-6pt}}
\setlength{\bibsep}{0pt} % or use whatever dimension you want
\renewcommand{\bibfont}{\fontsize{10pt}{12pt}\selectfont} % or any other  appropriate font command


\titleformat{\section}{\normalfont\large\bfseries\scshape}{}{0em}{}
\titlespacing*{\section}{0em}{0.5\baselineskip}{0em}
\titleformat{\subsection}[runin]{\normalfont\normalsize\bfseries}{}{0em}{}[.]
\titlespacing*{\subsection}{0em}{0.25\baselineskip}{0.5em}


%% LOGIC FOR CUSTOMIZING THE STATEMENT FOR INDIVIDUAL SCHOOLS
% set up common defines (commands and boolean flags)
\input{../defines.tex}
% import the school-specific header to make the magic happen!
\input{schoolheader.tex}
%% END CUSTOMIZATION LOGIC

\title{Towards Computational Methods for Assisting in the Proactive Moderation of Online Communities}

\begin{document}
\maketitle

\begin{center}
Research Statement --- Jonathan P. Chang
\end{center}

One of the biggest problems facing online communities today is the prevalence of incivility, harassment, hate speech, and other similar behaviors---collectively referred to in social science as \emph{antisocial} behavior.
Many popular online platforms try to address this problem by employing moderators to review content that gets posted and remove any content they judge to be antisocial behavior; consequently, a lot of Computer Science research on antisocial behavior tends to cater to this centralized moderation process.
But I believe that in developing computational tools from the perspective of large platform operators, we fail to center an even more important perspective: that of the \emph{communities} of ordinary people that interact on these platforms.
Therefore, my research focuses on the following question: how can technology help online communities to build consensus, maintain norms, and negotiate disputes---in other words, to \emph{proactively} prevent antisocial behavior from taking root?
I tackle this question through a combined social and technical research agenda, which involves engaging collaboratively with real online communities to identify their needs and the gaps in their computational toolkits, applying those insights to guide the development of algorithmic methods that might address those needs, and finally going back to the communities to collaboratively evaluate these new methods and identify future directions for research.

\section{Background: The Landscape of Moderation}

Popular discourse around content moderation has tended to focus on the practices of large social media companies like Facebook and Twitter, which employ a centralized, platform-driven model of moderation where company-employed moderators remove content deemed to be in violation of the platform's (often opaque) rules.
Yet a growing body of scholarship within social science and human-computer interaction argues that moderation actually encompasses a much richer body of practices which extend all the way down to the community level \cite{brewer_inclusion_2020,lampe_slashdot_2004,seering_reconsidering_2020}.
Community-driven moderation practices most often involve volunteer moderators, who differ from platform-employed moderators in several key ways.
Volunteer moderators are actual members of the communities they serve, and play a role akin to a community leader.
To this end, rather than relying solely on punitive steps like content removal, they also employ ``soft strategies'' like publicly modeling good behavior, educating community members about the rules, and mediating discussions that are getting out of hand \cite{seering_shaping_2017,billings_understanding_2010}.
Such steps can be seen as \emph{proactive} moderation aimed at discouraging antisocial behavior from occurring at all---standing in contrast to platform-employed moderators' practice of \emph{reactive} moderation, which acts in response to antisocial behavior after it occurs \cite{lo_when_2018}.


%Leveraging these logs, I conducted a data-driven analysis of how moderators interact with the editors whom they temporarily block, presented at the 2019 Web Conference (WWW 2019) \cite{chang_trajectories_2019}.
%A key finding was the importance of two-way trust in this relationship: blocked editors can signal trust in the moderator by acknowledging their wrongdoing (as measured through handwritten rules that detect actions such as apologies) while moderators can signal trust in the editor by granting requests for reduced block duration.
%Both such strategies are correlated with an increased likelihood of future rule-following by the blocked editor, showing how building trust may be a key factor in proactively fostering a prosocial, norm-adhering culture within an online community.


%These results point to an important aspect of community-driven moderation: it is very much a \emph{collaborative} project, one in which regular community members are not merely passive observers but instead play a direct role.
%Regular community members actively choose to accept---or not---the authority of volunteer moderators and the norms they promote, and may directly communicate their (un)acceptance to the moderators themselves.
%Volunteer moderators, for their part, seek to bolster their relationship of mutual trust with the community; in doing so, they may make community members more willing to adhere to rules and norms, thereby proactively reducing the likelihood of antisocial behavior.
%But how is this concretely achieved?
%My preceding data-driven analysis only reveals one strategy, the granting of clemency, which is applicable only in specific circumstances.
%Gaining a more complete picture of proactive moderation work requires taking a more holistic approach to analyzing the work of volunteer moderators as seen directly from their own perspectives.

%These results illuminate the collaborative nature of community-driven moderation: regular community members are a key contributor to the functioning of the system, as its success requires their buy-in and trust, which moderators in turn must foster.
%But besides grating clemency---which is a highly situational, niche strategy---how do volunteer moderators bolster their relationship with their community and proactively encourage prosocial behavior?
%To answer this question, I conducted interviews with Wikipedia moderators, in work published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%These interviews focused on the proactive side of volunteer moderators' work, asking questions about moderators reason about the likelihood of a stiuation deteriorating into antisocial behavior, and what kinds of proactive steps they take to prevent such outcomes.
%From the responses, a clear picture emerges of how volunteer moderators can tell when conflicts between Wikipedia editors are at risk of veering away from healthy disagreement and towards antisocial behavior, and the strategies they employ to get things back on track.

%To gain a broader view of proactive moderation strategies, I turned to interviews with Wikipedia moderators, which formed the basis of work presented at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
%All moderators in the interviews reported that they have some intuition for when a discussion is at risk of derailing into personal attacks or other prohibited antisocial behaviors.
%Furthermore, they described how when they discover such at-risk discussions, they may intervene to gently steer the conversation back on track, through techniques such as offering to mediate the disagreement.

\section{My Research Contributions}

\subsection{Identifying Community Moderation Practices and the Needs That Arise}
To gain an understanding of the needs that volunteer moderators face when doing their proactive work, I conducted interviews with moderators on Wikipedia's ``talk pages'' (a pseudo-forum where Wikipedia editors discuss and coordinate edits), resulting in findings that were published at the 2022 ACM Conference on Computer-Supported Collaborative Work (CSCW 2022) \cite{schluger_proactive_2022}.
These findings reveal a key challenge in proactive moderation: deciding where and when action is needed.
All moderators we interviewed told us they have some intuition about what makes a discussion at risk of derailing into future antisocial behavior, and that they use this intuition to determine when they might want to step in try to set the discussion back on track---for instance, by reminding the discussion participants about talk page conduct policies, or offering to serve as a neutral third party mediator.
But this intuition is far from perfect, with all moderators simultaneously indicating some degree of uncertainty about their intuitive judgements.
This difficulty is compounded by the large volume of discussions taking place at any given time, making it impossible to keep an eye on every single discussion that could turn out to be at risk.
Finally, even when a moderator decides a discussion is at risk and requires intervention, knowing exactly \emph{when} to intervene is a challenge of its own---due to the inherent uncertainty of predicting risk of derailment, moderators often hesitate to intervene too early, but if they wait too long the discussion may reach a point where it becomes too late to set things back on track.%; as one moderator poetically put it ``the hand grenade has gone off and I didn't even hear it because I've gone down the street''.

I believe that these challenges represent an opening where computational tools could be of assistance to volunteer moderators.
If an algorithm could capture some of the human intuition about when a discussion is at risk of future derailment, it could be used to automatically identify conversations that might require intervention, taking over some of the burden of manually finding such cases.
Additionally, such an algorithm could benefit regular community members by offering guidance on how their comments might affect the discussion and helping them avoid escalating at-risk situations, much like moderators' own soft interventions do.
Actually accomplishing this requires both developing this hypothetical derailment-forecasting algorithm, and doing design work to adapt it into a practical tool that is useful and, more importantly, safe---challenges which I have already begun to address.

\subsection{Evaluating the Feasibility of Algorithmically Forecasting Derailment}
The first step towards practical computational tools for proactive moderation is to show that it is feasible in the first place for an algorithm to pick up on signals of potential future derailment, like human moderators claim to be able to do.
To establish the feasibility of this task, we must look back to some of my earlier work on analyzing conversations, which focused on finding experimental evidence to support sociolinguistic theories about how to mediate tense conversations.

Classical literature in the field of pragmatics points to \emph{politeness} as a mediating factor that softens the perceived force of a message \cite{brown_politeness:_1987}.
In work presented at the 2018 Annual Meeting of the Association for Computational Linguistics (ACL 2018) \cite{zhang_conversations_2018}, my collaborators and I examined this theory computationally by building a software system that characterizes politeness by applying a combination of handwritten rules inspired by the classical theories \cite{danescu-niculescu-mizil_computational_2013} and an unsupervised method for estimating the rhetorical intent of an utterance \cite{zhang_asking_2017}.
We ran this system on comments sampled from Wikipedia talk page discussion, and compared the system-detected politeness features to the eventual outcome of the discussion (as labeled by human annotators).
Consistent with the classical theories, we confirmed that politness is correlated with reduced likelihood of antisocial outcomes.

More recent theoretical work has posited that misalignment between what a speaker \emph{intends} to communicate and how a listener \emph{perceives} them can spark conflict leading to negative outcomes \cite{tannen_indirectness_2000}.
Inspired by these theories, work conducted during an internship at Facebook and presented at the 2020 Web Conference (WWW 2020) \cite{chang_dont_2020}, my collaborators and I conducted a large-scale survey of Facebook users to obtain unprecedented ground-truth data about comment authors' intentions and readers' perceptions.
Through a regression analysis, we confirmed that discussions containing a misalignment between intentions and perceptions are more likely to end in antisocial behavior (as measured by an in-house moderation tool).

\subsection{Practical Forecasting of Conversational Derailment}
While my early work helps establish that an algorithm for forecasting derailment in conversations is feasible, it did not provide practically usable algorithms for such forecasting.
Achieving the latter requires overcoming two key practical challenges that are inherent to working with conversations.
First, conversations have \emph{unknown horizon}: rather than coming into existence fully-formed, online discussions grow and evolve over an extended period of time, which means a practical algorithm for forecasting derailment cannot just make a single prediction, but must instead follow a conversation as it develops in real time and update its predictions to reflect changes in the discussion.
Second, conversations are \emph{dynamic}: utterances in a discussion cannot be treated as isolated objects, because the meaning of an utterance often depends on context provided by previous utterances; a practical algorithm needs to be able to model such dependencies.

To address both challenges, I started with a key insight that these same challenges are faced by developers of dialog models (a.k.a. ``chatbots'')---like a forecasting algorithm, a dialog model also needs to follow a conversation in real time so that it can generate new replies on demand, and it needs to understand the relationships between utterances so it can generate situationally appropriate responses (e.g., knowing that some types of questions should be followed by an answer while other types are rhetorical).
Inspired by this insight, I adapted an existing neural network approach for dialog modeling \cite{serban_building_2016}: I first trained the network on the original dialog modeling task so it could learn conversational dynamics, then trained a classifier layer on top of the dialog model, which is used to produce a forecast (likelihood of derailment) after each new utterance.
The resulting model, which I have dubbed CRAFT, was presented at the ACL's 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019) \cite{chang_trouble_2019}.
Tested on the same Wikipedia talk page dataset as my earlier ACL 2018 paper, I showed that in 77\% of discussions that actually derail are correctly detected by CRAFT (the metric of recall); and conversely, across all conversations where CRAFT predicts derailment, 64\% in fact go on to derail (the metric of precision).
On average, CRAFT's first prediction of derailment in an actually-derailing conversation happens 3 comments before the outright antisocial comment (as identified by human annotators), giving a comfortable margin of advance warning.

\subsection{Using Forecasting Algorithms to Support Online Communities}
The introduction of CRAFT, a practical algorithm for forecasting derailment in conversations, made it possible to finally address a core question of my research agenda: how can such an algorithm, in its role as a rough proxy for human intuitions about risk, benefit real online communities?
Answering this question has required bringing together the social and technical aspects of my work.
On the technical side, I led a team of undergraduate and masters students to develop a prototype browser extension, ConvoWizard, that warns users when they are replying to a discussion thread that might be at risk of derailment (as estimated by CRAFT) and also gives them feedback as they draft their comment, warning them if the comment might make the situation even more tense.
On the social side, I arranged a groundbreaking collaboration with a real online community, the Reddit debate forum ChangeMyView, to help test ConvoWizard.

In setting up the collaboration with ChangeMyView, it was imporant to me to ensure that we were operating in accordance with the community's goals and needs, as opposed to imposing our own views of moderation on them.
As such, we worked together with the volunteer moderators of ChangeMyView to carefully design a user study that would be to the mutual benefit of both us and the ChangeMyView community.
We then jointly authored an announcement thread to inform the broader community of our goals and plans, also giving them a chance to weigh in with their own thoughts.
Interested users could opt in to participating in the study, where they had the chance to test ConvoWizard and provide feedback through a survey.

The results of the study, also published at CSCW 2022 \cite{chang_thread_2022}, offer promising indicators of a ConvoWizard-style tool's potential to improve online discussions.
We observed that when replying to at-risk threads, ConvoWizard users put more thought into their replies, spending 9\% more time writing when compared to a control condition.
More importantly, we found evidence that this extra time actually went towards trying to reduce tension in discussion: upon seeing warnings from ConvoWizard, users tended to make edits that decrease the risk of derailment (as estimated by CRAFT), unlike in the control condition where edits tended to increase the risk.
A linguistic analysis of the resulting comments showed that they tend to exhibit increased use of known conflict avoidance strategies such as asking questions and adopting more formal tone.
Finally, study participants' subjective evaluation of ConvoWizard in the exit survey was also positive, with 77\% of participants rating its feedback at least somewhat helpful, 68\% reporting that its risk estimates were as good as or better than their own intuitions, and 54\% reporting that ConvoWizard made them rethink posting a comment they might have later regretted.

A ConvoWizard-style tool, helping regular community members as they comment in online discussions, is only one half of my research agenda for proactive moderation tools.
As I have previously described, a second proposal is to use forecasting algorithms to directly help moderators by taking over some of the burden of identifying discussions that might need intervention.
I have already begun laying down the groundwork for this: part of my moderator interviews included a mockup interface for such a tool, involving a ranked list of discussions that are most at risk.
Moderators overall reacted positively to the idea and many stated they would be willing to try such a tool.
Building and evaluating a real implementation of such a tool is one part of my future plans, as I now discuss.

\section{Next Steps}
I am a firm believer in the principle that before new technologies can be broadly adopted, we must first try to understand as fully as possible its ramifications for society and its potential risks.
This is especially vital for work relating to content moderation, which is quickly becoming a major societal issue with implications for online safety, free speech, and social justice.
The social aspect of my work has been tailored towards understanding the impact of tools based on forecasting algorithms, and while I regard the findings from my interviews and the ConvoWizard user study as encouraging positive signs, we are still far from fully understanding the effects of these tools, and thus there is much work still to do.

One area that I particularly want to focus on is improving the transparency of forecasting algorithms.
My current approach, CRAFT, uses a neural network, a methodology whose decision processes are notoriously hard to explain.
Indeed, lack of transparency was the biggest complaint we received in the ConvoWizard exit survey, with users reporting that a warning about risk of derailment, with no explanation, is difficult to act upon even if they agree with it.
I regard this as the current biggest \emph{technical} hurdle to the usability of forecasting algorithms, and have already begun work on incorporating the latest research on explainable neural models to build a more transparent successor to CRAFT.
But this is not just short-term work: as the field of ``explainable AI'' grows, it will undoubtedly produce more breakthroughs that we can draw from, and I remain committed to pursuing this goal because I believe that if forecasting-based tools are ever broadly adopted, they must be based on explainable algorithms.

But the transparency issue is not just a technical problem: it also ties in to the larger \emph{social} problem of algorithmic bias.
By now, it has become well known that data-driven approaches are vulnerable to capturing biases embedded within the data.
The question, then, is not whether forecasting algorithms like CRAFT contain biases---the answer is surely yes---but rather what kinds of biases exist and what their impact would be in a broadly deployed forecasting-based tool.
This is uncharted territory, since most prior work on computational tools for moderation---and by extension, the analyses of their biases---focused on the reactive paradigm, and it is not clear how those findings might apply to proactive moderation.
Like the rest of my research agenda, my plan for addressing this question is to take a combined social and technical approach.
Once an explainable successor to CRAFT has been developed, my plan is to analyze the model's decisions through the lens of recent work from computational social science on codifying the social and power implications found in natural language \cite{sap_social_2020}.
Cross-referencing this work with the model's decision-making processes could reveal, for instance, whether the algorithm is less sensitive to attacks towards specific underrepresented groups.
And once such biases are identified, the same insights from social science could be applied to design ways to counter them.

It is also important once again to understand the impact of tools based on forecasting algorithms not just in the lab, but in the context of real communities.
All the future plans I just described should therefore be accompanied by further collaborative studies with real communities, including much larger-scale studies aimed at understanding long-term impacts, not just immediate implications on individual discussion threads.
As described earlier, these studies will also look into how forecasting algorithms might be used to directly support moderators, in additional to ConvoWizard-style tools aimed at regular community members.

\section{Involving Undergraduates in my Research Agenda}
Throughout my research, I have had the pleasure and privilege of collaborating with several undergraduate students, who have contributed to my research agenda in a number of ways.
Most notably, two of my papers---the two CSCW papers---were co-authored with undergraduate student Charlotte Schluger, who is in fact first author on one of them.
She had the distinction of contributing through the entire research pipeline: she was part of the larger student team that implemented the ConvoWizard research tool, was responsible for both designing and conducting the moderator interviews, and finally contributed large portions of the paper writing.
To me, Charlotte's experience illustrates the broad potential of my research agenda for undergraduates: it presents opportunities for both scientific contributions and software engineering contributions, thus catering to undergraduate students on any career path; indeed, other members of the ConvoWizard development team, knowing they wanted an industry job, chose to focus solely on the software engineering aspects.

My research agenda also opens up other paths for undergraduates to contribute beyond just work directly aimed at publications.
I have a strong commitment to open source and open science, and in this spirit I have been a core contributor to ConvoKit (\url{https://convokit.cornell.edu}), an open source Python package designed to empower research on conversational data.
As part of my work on ConvoKit I have mentored a number of undergraduate contributors, who again have gone onto a variety of career paths.
In fact, I met my first undergraduate mentee, Andrew Wang, through his work on ConvoKit, which he used as a launching point to get into research; he would later go on to receive a MS degree at Stanford followed by returning to Cornell to pursue a PhD.
On a different type of career path is Caleb Chiam, who over his 3 years working with us as an undergraduate became one of the biggest contributors to ConvoKit---experience which now serves him well at his current software engineering career.

Overall, I believe that my research agenda is a perfect fit for an undergraduate teaching institution.
Because of my focus on practical use cases, my research involves a heavy software engineering focus alongside the typical scientific aspects.
As my prior experiences with undergraduate students have shown, this means that my research has something to offer for everyone---a particularly important feature at an undergraduate institution, where students may be pursuing a variety of different post-graduation trajectories, or in fact trying to decide on one.
My continued commitment to open source and the ConvoKit project also offers a unique opportunity for students to have an impact on a real tool that is used by researchers and software engineers while simultaneously getting exposure to research.
In light of all of this, I feel that I can strongly contribute to the education of students at UNIVERSITY\_NAME.

\vspace{\baselineskip}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
